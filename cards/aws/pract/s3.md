#flashcards #aws

status:: 
count:: 
[[Flashcards]]
Imports

Here is a Markdown file containing a dozen flashcards based on the Amazon S3 content you provided:

 
<!-- Card Start -->

###   S3

**What is Amazon S3 and what are some of its common use cases?**


### Back

**Amazon S3** (Simple Storage Service) is an object storage service that allows you to store and retrieve any amount of data from anywhere on the web. Common use cases include:

- Backup and storage
- Disaster Recovery
- Archive
- Hybrid Cloud storage
- Application hosting
- Media hosting
- Data lakes & big data analytics
- Software delivery
- Static website hosting

<!-- Card End -->

---
<!-- Card Start -->

### Front  

**What is an S3 Bucket, and what are the naming conventions for buckets?**

- Hint: Think about how S3 organizes data.

### Back

An **S3 Bucket** is a container for storing objects (files) in Amazon S3. Buckets are defined at the region level and must have a globally unique name.

**Naming Conventions:**

- No uppercase letters
- No underscores
- 3-63 characters long
- Not formatted as an IP address
- Must start with a lowercase letter or number

Example: `my-bucket`

<!-- Card End -->

---
<!-- Card Start -->

### Front

**Explain the concept of S3 Objects and Keys. How are objects organized in S3?**

 

### Back

**S3 Objects** are the fundamental entities stored in Amazon S3, consisting of object data and metadata.

- Each object is identified by a **Key**, which is the full path to the object, including any prefixes (like folders).

Example Key:

```
s3://my-bucket/my_folder1/another_folder/my_file.txt
```

- **Prefixes** are used to organize objects in a flat structure; there are no true directories in S3.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**How is security implemented in Amazon S3?**

 

### Back

**Security in Amazon S3** is implemented through:

- **User-Based Policies:**
    
    - **IAM Policies**: Define which API calls are allowed for specific IAM users or roles.
- **Resource-Based Policies:**
    
    - **Bucket Policies**: JSON-based policies that define access permissions for the entire bucket.
    - **Bucket Access Control Lists (ACLs)**: Grant permissions at the bucket level.
    - **Object ACLs**: Grant permissions at the object level.
- **Encryption**: Objects can be encrypted using server-side or client-side encryption methods.
    

**Access is granted if:**

- The IAM user has the necessary permissions **OR**
- The resource policy allows it **AND**
- There is no explicit **DENY** in any policy.

<!-- Card End -->

---
<!-- Card Start -->

### Front

**What are S3 Bucket Policies, and how are they structured?**

- Hint: Think about the components of a policy.

### Back

**S3 Bucket Policies** are JSON-based resource policies that define access permissions for S3 buckets and the objects within them.

**Structure:**

- **Version**: Policy language version.
    
- **Statement**: One or more individual statements.
    
    - **Sid**: Optional statement ID.
    - **Effect**: `Allow` or `Deny`.
    - **Principal**: The account or user to which the policy applies.
    - **Action**: List of actions that are allowed or denied.
    - **Resource**: Buckets and objects to which the actions apply.

**Uses:**

- Grant public or cross-account access.
- Enforce encryption during upload.
- Define granular access controls.

<!-- Card End -->

---
<!-- Card Start -->

### Front

**What is the purpose of S3 Block Public Access settings?**

- Hint: Think about preventing unintended data exposure.

### Back

**S3 Block Public Access** settings are used to prevent public access to S3 buckets and objects, regardless of the bucket policies or object permissions.

**Key Settings:**

- **Block Public ACLs**: Prevent new public ACLs on buckets and objects.
- **Ignore Public ACLs**: Ignore existing public ACLs.
- **Block Public Bucket Policies**: Prevent new public bucket policies that allow public access.
- **Restrict Public Bucket Policies**: Restrict access to buckets with public policies.

**Purpose:**

- Enhance security by preventing accidental public exposure of data.
- Recommended to leave these settings **ON** for buckets that should not be publicly accessible.

<!-- Card End -->

---
<!-- Card Start -->

### Front 
**How can Amazon S3 be used to host static websites, and what URL format is used to access them?**

- Hint: Think about the bucket configuration and public access.

### Back

**Hosting Static Websites on S3:**

- Enable static website hosting in the bucket properties.
- Upload static files (HTML, CSS, JS) to the bucket.
- Configure the bucket policy to allow public read access to the objects.

**Website URL Format:**

- `http://<bucket-name>.s3-website-<AWS-region>.amazonaws.com`
- Example: `http://my-bucket.s3-website-us-east-1.amazonaws.com`

**Note:** If you receive a **403 Forbidden** error, ensure that the bucket policy allows public read access.

<!-- Card End -->

---
<!-- Card Start -->

### Front  

**What is S3 Versioning, and what are its benefits?**

- Hint: Think about object states and accidental deletions.

### Back

**S3 Versioning** is a feature that keeps multiple versions of an object in the same bucket.

**Benefits:**

- **Data Protection**: Protects against accidental overwrites and deletions.
- **Easy Recovery**: Allows you to retrieve and restore previous versions of objects.
- **Best Practice**: Recommended to enable versioning to safeguard data.

**Notes:**

- Versioning is enabled at the bucket level.
- Once enabled, all new objects will have a unique version ID.
- Suspending versioning does not delete existing versions.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**What are S3 Access Logs, and why are they useful?**



### Back

**S3 Access Logs** provide detailed records of the requests made to an S3 bucket.

**Uses:**

- **Audit Purposes**: Track who is accessing your data and how.
- **Security Analysis**: Detect unauthorized access attempts.
- **Operational Monitoring**: Analyze usage patterns and performance.

**Implementation:**

- Access logs are stored in another S3 bucket.
- Can be analyzed using data analysis tools like AWS Athena or third-party solutions.

<!-- Card End -->

---
<!-- Card Start -->

### Front   

**What is S3 Replication, and what are the differences between Cross-Region Replication (CRR) and Same-Region Replication (SRR)?**

- Hint: Think about use cases and requirements.

### Back

**S3 Replication** automatically copies objects from one S3 bucket to another.

**Types:**

- **Cross-Region Replication (CRR):**
    
    - Replicates objects to a bucket in a different AWS region.
    - **Use Cases:** Compliance requirements, latency reduction, data sovereignty.
- **Same-Region Replication (SRR):**
    
    - Replicates objects to a bucket within the same AWS region.
    - **Use Cases:** Log aggregation, backup between production and test accounts.

**Requirements:**

- Versioning must be enabled on both source and destination buckets.
- Appropriate IAM permissions must be set for replication.

**Notes:**

- Replication is asynchronous.
- Buckets can be in different AWS accounts.

<!-- Card End -->

---
<!-- Card Start -->

### Front
**List and briefly describe the different S3 Storage Classes.**

- Hint: Consider cost and access patterns.

### Back

**S3 Storage Classes:**

1. **S3 Standard - General Purpose:**
    
    - High durability (99.999999999%) and availability (99.99%).
    - Designed for frequently accessed data.
2. **S3 Standard-Infrequent Access (S3 Standard-IA):**
    
    - Lower storage cost, higher retrieval cost.
    - For data accessed less frequently but requires rapid access.
3. **S3 One Zone-Infrequent Access (S3 One Zone-IA):**
    
    - Data stored in a single Availability Zone.
    - Suitable for secondary backups or easily recreatable data.
4. **S3 Glacier Instant Retrieval:**
    
    - Low-cost storage for data that is rarely accessed but requires milliseconds retrieval.
5. **S3 Glacier Flexible Retrieval:**
    
    - For archival data with retrieval times from minutes to hours.
6. **S3 Glacier Deep Archive:**
    
    - Lowest-cost storage for long-term retention with retrieval times in hours.
7. **S3 Intelligent-Tiering:**
    
    - Automatically moves data between tiers based on usage patterns.
    - No retrieval fees.

**Note:** All storage classes offer the same high durability.

<!-- Card End -->

---
<!-- Card Start -->

### Front

**What is S3 Intelligent-Tiering, and how does it optimize storage costs?**

- Hint: Think about automatic data movement.

### Back

**S3 Intelligent-Tiering** is a storage class that automatically moves objects between different access tiers based on changing access patterns.

**Features:**

- **Frequent Access Tier**: Default tier for newly uploaded data.
- **Infrequent Access Tier**: Data not accessed for 30 consecutive days moves here.
- **Archive Instant Access Tier**: Data not accessed for 90 days.
- **Optional Archive Tiers**: Archive Access and Deep Archive Access for long-term storage.

**Benefits:**

- **Cost Optimization**: Reduces storage costs by automatically moving data to the most cost-effective tier.
- **No Retrieval Fees**: Unlike other infrequent access tiers, there are no retrieval charges.
- **Monitoring Fee**: Small monthly fee for monitoring and automation.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**Explain S3 Object Lock and Glacier Vault Lock. When would you use them?**

 

### Back

**S3 Object Lock:**

- Enables a **Write Once Read Many (WORM)** model.
- Prevents object version deletion or modification for a specified retention period.
- **Use Cases:** Regulatory compliance, data retention policies.

**Glacier Vault Lock:**

- Also implements a **WORM** model.
- Allows you to lock the vault's policy to prevent future changes.
- **Use Cases:** Long-term compliance archives, audit requirements.

**Benefits:**

- Ensures data cannot be altered or deleted during the retention period.
- Helps meet compliance standards like SEC Rule 17a-4(f).

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**What is the AWS Snow Family, and what are its primary use cases?**

- Hint: Think about data migration and edge computing.

### Back

The **AWS Snow Family** consists of physical devices used for data migration and edge computing in environments with limited connectivity.

**Devices:**

- **AWS Snowcone:**
    
    - Smallest device with 8 TB usable storage.
    - Used for data transfer and edge computing in constrained environments.
- **AWS Snowball Edge:**
    
    - Comes in **Storage Optimized** and **Compute Optimized** variants.
    - Used for large-scale data migrations and edge computing.
- **AWS Snowmobile:**
    
    - Massive data transfer device with up to 100 PB capacity.
    - Used for exabyte-scale data migrations.

**Use Cases:**

- **Data Migration:** Moving large amounts of data into or out of AWS when network transfer is impractical.
- **Edge Computing:** Processing data at the edge in remote or disconnected environments.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**Describe the process of migrating data using AWS Snowball Edge.**

- Hint: Think about the steps from request to data upload.

### Back

**AWS Snowball Edge Data Migration Process:**

1. **Order Device:** Request a Snowball Edge device via the AWS Management Console.
    
2. **Receive Device:** AWS ships the device to your on-premises location.
    
3. **Set Up Device:**
    
    - Install the **AWS OpsHub** software or use the Snowball client.
    - Unlock and configure the device.
4. **Transfer Data:**
    
    - Connect the device to your local network.
    - Copy data to the device using NFS, S3 API, or other supported protocols.
5. **Return Device:**
    
    - Ship the device back to AWS using the provided shipping label.
6. **Data Ingestion:**
    
    - AWS receives the device and uploads the data to your specified S3 bucket.
7. **Data Verification:**
    
    - Data is verified and the device is securely wiped.

**Benefits:**

- **Efficient Transfers:** Avoids network bottlenecks and high transfer costs.
- **Security:** Data is encrypted during transit and storage on the device.

<!-- Card End -->

---
<!-- Card Start -->

### Front

**What is AWS Storage Gateway, and what are its different types?**

- Hint: Think about hybrid cloud storage solutions.

### Back

**AWS Storage Gateway** is a hybrid cloud storage service that provides on-premises access to virtually unlimited cloud storage.

**Types of Storage Gateway:**

1. **File Gateway:**
    
    - Presents a file system interface (NFS/SMB) to on-premises applications.
    - Files are stored as objects in S3.
2. **Volume Gateway:**
    
    - Provides block storage using iSCSI protocol.
    - Backed by Amazon EBS snapshots.
3. **Tape Gateway:**
    
    - Emulates a physical tape library for backup applications.
    - Virtual tapes are stored in S3 and archived in Glacier.

**Use Cases:**

- **Hybrid Cloud Storage:** Seamlessly integrate on-premises environments with AWS storage.
- **Backup and Restore:** Efficiently back up data to AWS.
- **Disaster Recovery:** Store copies of critical data in the cloud.

<!-- Card End -->

---
 

---
<!-- Card Start -->

### Front 

**What is the maximum object size you can upload to Amazon S3, and what should you do if you need to upload larger objects?**


### Back

- The **maximum object size** you can upload in a single PUT operation is **5 GB**.
- For objects larger than **5 GB** (up to **5 TB**), you must use **Multi-Part Upload**.
    - This involves breaking the object into smaller parts and uploading them concurrently.
- **Benefits of Multi-Part Upload:**
    - Improved upload performance.
    - Ability to upload parts in parallel.
    - Resilience to network issues; you can retry failed parts individually.

<!-- Card End -->

---
<!-- Card Start -->

### Front

**Explain the Shared Responsibility Model for Amazon S3. What are AWS's responsibilities versus the customer's responsibilities?**

- Hint: Think about infrastructure versus configuration.

### Back

**Shared Responsibility Model for Amazon S3:**

- **AWS Responsibilities:**
    
    - Infrastructure management (global security, durability, availability).
    - Maintaining the underlying hardware and software.
    - Ensuring physical security of data centers.
- **Customer Responsibilities:**
    
    - **Data Protection:**
        - Implementing **S3 Versioning**.
        - Managing **S3 Bucket Policies** and **Access Control Lists**.
        - Configuring **S3 Replication** if needed.
    - **Security Configuration:**
        - Encrypting data at rest and in transit.
        - Managing **IAM policies** for user access.
    - **Monitoring and Compliance:**
        - Setting up **Logging and Monitoring** (e.g., CloudTrail, S3 Access Logs).
        - Choosing appropriate **S3 Storage Classes** based on use case.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**What are S3 Lifecycle Rules, and how can they help manage costs?**



### Back

**S3 Lifecycle Rules** allow you to define actions to automatically transition objects to different storage classes or expire them after a specified period.

**Benefits:**

- **Cost Management:**
    - Automatically move objects to cheaper storage classes (e.g., from S3 Standard to S3 Glacier) based on access patterns.
- **Data Retention Policies:**
    - Define when objects should be deleted, helping to manage storage costs and comply with data retention regulations.

**Common Actions:**

- **Transition Actions:**
    - Move objects to Infrequent Access classes after a period of inactivity.
- **Expiration Actions:**
    - Permanently delete objects after a specified time.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**What is AWS Snowball Edge, and what are the differences between the Storage Optimized and Compute Optimized versions?**

- Hint: Think about capacity and use cases.

### Back

**AWS Snowball Edge** is a physical device for data transfer and edge computing.

**Versions:**

1. **Snowball Edge Storage Optimized:**
    
    - **Storage Capacity:** 80 TB usable.
    - **Compute:** Up to 40 vCPUs, 80 GiB RAM.
    - **Use Cases:** Large-scale data migrations, local storage and processing.
2. **Snowball Edge Compute Optimized:**
    
    - **Storage Capacity:** 42 TB usable.
    - **Compute:** 52 vCPUs, 208 GiB RAM.
    - **Optional GPU:** For advanced computing tasks.
    - **Use Cases:** Machine learning, full-motion video analysis, advanced analytics.

**Common Features:**

- Can run **EC2 instances** and **AWS Lambda functions**.
- Supports **edge computing** workloads.

<!-- Card End -->

---
<!-- Card Start -->

### Front

**How does Amazon S3 ensure high durability for stored objects?**

- Hint: Think about data replication across Availability Zones.

### Back

Amazon S3 achieves **11 nines of durability (99.999999999%)** by:

- **Automatically replicating data across multiple Availability Zones (AZs)** within a region.
- **Data Integrity Checks:**
    - Regularly verifying the integrity of data using checksums.
    - Repairing corrupted data using redundant copies.
- **Redundant Storage:**
    - Storing multiple copies of data to prevent loss due to hardware failures.

This means that if you store 10 million objects, you can expect to lose one object every 10,000 years on average.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**What is Amazon S3 Glacier, and what are the retrieval options available?**

- Hint: Think about archival storage and retrieval times.

### Back

**Amazon S3 Glacier** is a low-cost storage class designed for data archiving.

**Retrieval Options:**

1. **Glacier Instant Retrieval:**
    
    - Millisecond retrieval.
    - Suitable for data accessed once per quarter.
2. **Glacier Flexible Retrieval (formerly Glacier):**
    
    - **Expedited Retrieval:** 1-5 minutes.
    - **Standard Retrieval:** 3-5 hours.
    - **Bulk Retrieval:** 5-12 hours (free).
3. **Glacier Deep Archive:**
    
    - **Standard Retrieval:** Up to 12 hours.
    - **Bulk Retrieval:** Up to 48 hours.
    - Lowest storage cost, suitable for long-term retention.

**Use Cases:**

- Long-term backups.
- Compliance archives.
- Data that rarely needs to be accessed.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**Explain the purpose and benefits of Amazon S3 Transfer Acceleration.**


### Back

**Amazon S3 Transfer Acceleration** enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket.

**How It Works:**

- Utilizes **Amazon CloudFront's globally distributed edge locations**.
- Data is routed to the nearest edge location and then transferred to the S3 bucket over optimized network paths.

**Benefits:**

- **Improved Transfer Speeds:** Especially effective for uploads over long distances or in regions with poor connectivity.
- **No Additional Infrastructure:** Easy to enable on an existing bucket without changing application code.
- **Security:** Data is encrypted during transit.

<!-- Card End -->

---
<!-- Card Start -->

### Front

**What are S3 Event Notifications, and how can they be used?**

### Back

**S3 Event Notifications** allow you to receive notifications when specific events happen in your S3 bucket.

**Supported Destinations:**

- **Amazon Simple Notification Service (SNS) Topics**
- **Amazon Simple Queue Service (SQS) Queues**
- **AWS Lambda Functions**

**Use Cases:**

- **Triggering Workflows:** Automatically process images, videos, or documents when they are uploaded.
- **Data Processing Pipelines:** Initiate ETL jobs when new data arrives.
- **Monitoring and Alerts:** Receive notifications for object creation, deletion, or replication failures.

**Configuration:**

- Specify the bucket events to monitor (e.g., `s3:ObjectCreated:*`).
- Set up the destination for the notifications.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**What is the purpose of Amazon S3 Access Points, and how do they simplify managing data access?**



### Back

**Amazon S3 Access Points** are network endpoints that simplify managing data access at scale for shared datasets in S3.

**Benefits:**

- **Custom Permissions:** Each access point can have its own IAM policies and network configurations.
- **Simplified Management:** Easier to manage access for different applications, teams, or use cases.
- **VPC Integration:** Restrict access to a Virtual Private Cloud (VPC) for enhanced security.

**Use Cases:**

- **Large Data Lakes:** Manage access for multiple users and applications.
- **Multi-Tenant Environments:** Isolate access per tenant or customer.

**Key Features:**

- Support for bucket-level settings like versioning and encryption.
- Can coexist with existing bucket policies.

<!-- Card End -->

---
<!-- Card Start -->

### Front 
**How does Amazon S3 handle data consistency for read and write operations?**

 

### Back

**Data Consistency Model in Amazon S3:**

- **Strong Read-After-Write Consistency for PUTs and DELETEs of Objects:**
    
    - After a successful write (PUT or DELETE), any subsequent read request immediately receives the latest version.
    - Applies to both new objects and overwrites.
- **List Operations:**
    
    - Changes in object listing (e.g., when listing objects in a bucket) are **eventually consistent**.

**Benefits:**

- Simplifies application development by providing predictable read behavior.
- Ensures that you always read the latest data after a write operation.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**What is AWS DataSync, and how does it relate to Amazon S3?**
 
### Back

**AWS DataSync** is a data transfer service that simplifies, automates, and accelerates moving data between on-premises storage and AWS storage services.

**Relation to Amazon S3:**

- **Transfer Data To/From S3:**
    - Migrate or replicate data between on-premises storage and Amazon S3.
    - Supports S3 buckets, including S3 Glacier storage classes.

**Features:**

- **Automated Transfers:** Schedule and monitor data transfers.
- **Optimized Performance:** Uses a purpose-built protocol to accelerate data movement.
- **Integrity Verification:** Ensures data is transferred securely and accurately.

**Use Cases:**

- **Data Migration:** Move large datasets to the cloud.
- **Data Replication:** Keep on-premises and cloud storage in sync.
- **Backup and Archiving:** Regularly back up data to Amazon S3.

<!-- Card End -->

---
<!-- Card Start -->

### Front 

**Explain how Amazon S3 Object Tags work and their use cases.**

### Back

**Amazon S3 Object Tags** are key-value pairs applied to S3 objects.

**Features:**

- **Metadata Management:** Store additional information about the object.
- **Supports Up to 10 Tags per Object**

**Use Cases:**

- **Access Control:** Implement fine-grained access controls using tags in IAM policies.
- **Lifecycle Management:** Configure lifecycle rules to manage objects based on tags (e.g., transition to another storage class, expiration).
- **Cost Allocation:** Track storage costs by tagging objects with project or department identifiers.
- **Data Classification:** Categorize data for analysis or compliance purposes.

**Example Tagging Policy:**

- Key: `Project`, Value: `Alpha`
- Key: `Environment`, Value: `Production`

<!-- Card End -->

---
<!-- Card Start -->

### Front S3  

**What is Amazon S3 Select, and how does it improve performance when retrieving data?**

 
### Back

**Amazon S3 Select** enables you to retrieve a subset of data from **an object** using SQL expressions.

note its one object  athena is the large scale deal
**Benefits:**

- **Improved Performance:**
    
    - Reduces the amount of data transferred by retrieving only the required data.
    - Speeds up applications that need to access specific data within objects.
- **Cost Savings:**
    
    - Decreases data transfer costs and processing overhead.

**Use Cases:**

- **Log Processing:** Extract specific fields from log files stored in S3.
- **Data Analysis:** Retrieve certain rows or columns from large CSV or JSON files.
- **Machine Learning:** Preprocess data by selecting relevant subsets.

**Supported Formats:**

- CSV
- JSON
- Apache Parquet

**Example SQL Query:**

```sql
SELECT s.name, s.age FROM S3Object s WHERE s.age > 30
```

<!-- Card End -->

---
 